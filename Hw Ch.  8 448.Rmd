---
title: "Hw Ch. 8 MATH 338"
output:
  pdf_document: default
  html_document: default
  word_document: default
---
#4 
```{r}
#a. Idk how to draw this in R, please refer to part b for the similar diagram. 
#b.
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(-2, 2), ylim = c(-3, 3), xlab = "X1", ylab = "X2")

#x2<1
lines(x = c(-2, 2), y = c(1, 1))

#x1<1 with x2<1
lines(x = c(1, 1), y = c(-3, 1))
text(x = (-2 + 1)/2, y = -1, labels = c(-1.8))
text(x = 1.5, y = -1, labels = c(0.63))

#x2<2 with x2>= 1
lines(x = c(-2, 2), y = c(2, 2))
text(x = 0, y = 2.5, labels = c(2.49))

#x1<0 with x2<2 and x2>=1
lines(x = c(0, 0), y = c(1, 2))
text(x = -1, y = 1.5, labels = c(-1.06))
text(x = 1, y = 1.5, labels = c(0.21))

```


#8
```{r}
#a. 
install.packages("ISLR", repos = "https://cran.rstudio.com")
library(ISLR)
attach(Carseats)
set.seed(10)

##split data##
train = sample(dim(Carseats)[1], dim(Carseats)[1]/2)
Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]

#b.
install.packages("tree", repos = "https://cran.rstudio.com")
library(tree)

##regression tree##
tree.car = tree(Sales ~ ., data = Carseats.train)
summary(tree.car)

##plot of rt##
plot(tree.car)
text(tree.car, pretty = 0)

##MSE##
pred.carseats = predict(tree.car, Carseats.test)
mean((Carseats.test$Sales - pred.carseats)^2)
###Test MSE is about 5.2

#c. 
cv.carseats = cv.tree(tree.car, FUN = prune.tree)
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")

##best size looks to be 6 
pruned.carseats = prune.tree(tree.car, best = 6)
par(mfrow = c(1, 1))
plot(pruned.carseats)
text(pruned.carseats, pretty = 0)

pred.pruned = predict(pruned.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.pruned)^2)
##Test MSE is about 4.86 
##By pruning the data we were successfuly able to decrease the test MSE.

#d. 
install.packages("randomForest", repos = "https://cran.rstudio.com")
library(randomForest)

##MSE##
bag.car = randomForest(Sales ~ ., data = Carseats.train, mtry = 10, 
                            ntree = 500, importance = T)
bag.pred = predict(bag.car, Carseats.test)
mean((Carseats.test$Sales - bag.pred)^2)
##MSE is improved tremendously after bagging. MSE is aboutt 

importance(bag.car)
##seems that the most important predictors of Sale are are CompPrice, ShelveLoc,
##and Price. 

#e. 
oob.error = double(10)
test.error = double(10)
for(mtry in 1:10)
{
  rf.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = mtry, 
                             ntree = 500, importance = T)
  oob.error[mtry] = rf.carseats$mse[500]
  rf.pred = predict(rf.carseats, Carseats.test)
  test.error[mtry] = mean((Carseats.test$Sales - rf.pred)^2)
  cat(mtry," ")
}
matplot(1:mtry, cbind(test.error, oob.error), pch = 19, 
    col = c("darkorchid3","deeppink3"), type = "b", ylab = "Mean Squared Error")
##9 is the best mtry value 

importance(rf.carseats)
##Same important predictors from part d. 
test.error[which.min(test.error)]
##Test MSE also very similar to part d @ 2.77
##changing the m value changes our test error, the test error between 2.5 and 3.

```


#10
```{r}
#a.
library(ISLR)

#Cleanse data
sum(is.na(Hitters$Salary))

Hitters = Hitters[-which(is.na(Hitters$Salary)), ]
sum(is.na(Hitters$Salary))
Hitters$Salary = log(Hitters$Salary)

#b. 
train = 1:200
Hitters.train = Hitters[train, ]
Hitters.test = Hitters[-train, ]

#c. 
install.packages("gbm", repos = "https://cran.rstudio.com")
library(gbm)
set.seed(10)
pows = seq(-10, -0.2, by = 0.1)
lambdas = 10^pows
length.lambdas = length(lambdas)
train.errors = rep(NA, length.lambdas)
test.errors = rep(NA, length.lambdas)
for (i in 1:length.lambdas) 
  {
    boost.hitters = gbm(Salary ~ ., data = Hitters.train, distribution = 
                          "gaussian", 
        n.trees = 1000, shrinkage = lambdas[i])
    train.pred = predict(boost.hitters, Hitters.train, n.trees = 1000)
    test.pred = predict(boost.hitters, Hitters.test, n.trees = 1000)
    train.errors[i] = mean((Hitters.train$Salary - train.pred)^2)
    test.errors[i] = mean((Hitters.test$Salary - test.pred)^2)
}
##train
plot(lambdas, train.errors, type = "b", xlab = "Shrinkage", ylab = "Train MSE", 
    col = "blue")

#d. 
##test
plot(lambdas, test.errors, type = "b", xlab = "Shrinkage", ylab = "Test MSE", 
    col = "orange")
#e. 
##Linear 
lm.fit = lm(Salary ~ ., data = Hitters.train)
lm.pred = predict(lm.fit, Hitters.test)
mean((Hitters.test$Salary - lm.pred)^2)

##Lasso
install.packages("glmnet", repos = "https://cran.rstudio.com")
library(glmnet)

set.seed(10)

x = model.matrix(Salary ~ ., data = Hitters.train)
y = Hitters.train$Salary
x.test = model.matrix(Salary ~ ., data = Hitters.test)
lasso.fit = glmnet(x, y, alpha = 1)
lasso.pred = predict(lasso.fit, s = 0.01, newx = x.test)
mean((Hitters.test$Salary - lasso.pred)^2)
##both linear and lasso show increased test MSE's as compared to boosting. 

#f.
boost.best = gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", 
    n.trees = 1000, shrinkage = lambdas[which.min(test.errors)])
summary(boost.best)
##CHits, CAtBat, and CRBI were the most important predictors in boosted model. 

#g. 
library(randomForest)

set.seed(10)
#increased mtry for bagging + boosting
rf.hitters = randomForest(Salary ~ ., data = Hitters.train, ntree = 500, 
                          mtry = 19)
rf.pred = predict(rf.hitters, Hitters.test)
mean((Hitters.test$Salary - rf.pred)^2)
##test MSE is about .232 which is lower than best test MSE for boosting. 
```

